{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bbd33-98d3-4f9d-9cdb-39f53d357d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check requirements.txt for list of necessary libraries to run this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80951bea-6a20-4fbf-911b-22330e516c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf4cab-66fb-439c-b627-7bac4dd242c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operacje architektury U-net\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        p = self.pool(down)\n",
    "\n",
    "        return down, p\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "       x1 = self.up(x1)\n",
    "       x = torch.cat([x1, x2], 1)\n",
    "       return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a90fbc-f442-430c-b0be-4be61a52d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up U-Net\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.down_convolution_1 = DownSample(in_channels, 64)\n",
    "        self.down_convolution_2 = DownSample(64, 128)\n",
    "        self.down_convolution_3 = DownSample(128, 256)\n",
    "        self.down_convolution_4 = DownSample(256, 512)\n",
    "\n",
    "        self.bottle_neck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up_convolution_1 = UpSample(1024, 512)\n",
    "        self.up_convolution_2 = UpSample(512, 256)\n",
    "        self.up_convolution_3 = UpSample(256, 128)\n",
    "        self.up_convolution_4 = UpSample(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "       down_1, p1 = self.down_convolution_1(x)\n",
    "       down_2, p2 = self.down_convolution_2(p1)\n",
    "       down_3, p3 = self.down_convolution_3(p2)\n",
    "       down_4, p4 = self.down_convolution_4(p3)\n",
    "\n",
    "       b = self.bottle_neck(p4)\n",
    "\n",
    "       up_1 = self.up_convolution_1(b, down_4)\n",
    "       up_2 = self.up_convolution_2(up_1, down_3)\n",
    "       up_3 = self.up_convolution_3(up_2, down_2)\n",
    "       up_4 = self.up_convolution_4(up_3, down_1)\n",
    "\n",
    "       out = self.out(up_4)\n",
    "       return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6887fd-afd9-4d05-9881-4afc3c41c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset class, used to move across the folder with images of lakes/lake, change paths in accordance with path example in comment within if statement below\n",
    "#in order to change file directories to move across, reccommended image resolution is 512 by 512 pixels for minimal loss of accuracy\n",
    "#all images will be resized to this resolution otherwise which may result in a varried amount of loss of accuracy\n",
    "class mldsjeziora(Dataset):\n",
    "    def __init__(self, root_path, test=False):\n",
    "        self.root_path = root_path\n",
    "        if test:\n",
    "            #self.images = sorted([root_path+\"/test/image/\"+i for i in os.listdir(root_path+\"/test/image/\")])\n",
    "            #self.masks = sorted([root_path+\"/test/masks/\"+i for i in os.listdir(root_path+\"/test/masks/\")])\n",
    "            self.images = sorted([root_path+\"your_image_folder\"+i for i in os.listdir(root_path+\"your_image_folder\")])\n",
    "            self.masks = sorted([root_path+\"your_mask_folder\"+i for i in os.listdir(root_path+\"your_mask_folder\")])\n",
    "        else:\n",
    "            self.images = sorted([root_path+\"your_image_folder\"+i for i in os.listdir(root_path+\"your_image_folder\")])\n",
    "            self.masks = sorted([root_path+\"your_mask_folder\"+i for i in os.listdir(root_path+\"your_mask_folder\")])\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert(\"L\")\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        mask = Image.open(self.masks[index]).convert(\"L\")\n",
    "\n",
    "        return self.transform(img), self.transform(mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d72be-bfce-4b7e-8f46-5b59506cea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum class used to calculate pixel based surface area of the lake from the mask/model output\n",
    "def summation(test_tup):\n",
    " \n",
    "    # Converting into list\n",
    "    test = list(test_tup)\n",
    " \n",
    "    # Initializing count\n",
    "    count = 0\n",
    " \n",
    "    # for loop\n",
    "    for i in test:\n",
    "        count += i\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee935ffc-b92c-4390-a4fa-db71b8021660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to check images with the model and compare with test data prepared ahead of time\n",
    "def pred_show_image_grid(data_path, model_pth, device):\n",
    "    model = UNet(in_channels=3, num_classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_pth, weights_only=True, map_location=torch.device(device)))\n",
    "    image_dataset = mldsjeziora(data_path, test=True)\n",
    "    images = []\n",
    "    orig_masks = []\n",
    "    pred_masks = []\n",
    "    total_lake_sa = 0\n",
    "    total_mask_sa = 0\n",
    "    for img, orig_mask in image_dataset:\n",
    "        img = img.float().to(device)\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        pred_mask = model(img)\n",
    "\n",
    "        img = img.squeeze(0).cpu().detach()\n",
    "        img = img.permute(1, 2, 0)\n",
    "        \n",
    "        pred_mask = pred_mask.squeeze(0).cpu().detach()\n",
    "        pred_mask = pred_mask.permute(1, 2, 0)\n",
    "        \n",
    "        pred_mask[pred_mask < 0.81]=0\n",
    "        pred_mask[pred_mask > 0.81]=1\n",
    "        \n",
    "        orig_mask = orig_mask.cpu().detach()\n",
    "        orig_mask = orig_mask.permute(1, 2, 0)\n",
    "\n",
    "        images.append(img)\n",
    "        orig_masks.append(orig_mask)\n",
    "        pred_masks.append(pred_mask)\n",
    "\n",
    "        lake_sa = sum(pred_mask)\n",
    "        total_lake_sa = total_lake_sa + lake_sa\n",
    "        orig_mask_sa = sum(orig_mask)\n",
    "        total_mask_sa = total_mask_sa + orig_mask_sa\n",
    "    \n",
    "    images.extend(orig_masks)\n",
    "    images.extend(pred_masks)\n",
    "    fig = plt.figure()\n",
    "    for i in range(1, 3*len(image_dataset)+1):\n",
    "       fig.add_subplot(3, len(image_dataset), i)\n",
    "       plt.imshow(images[i-1], cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "    #calculations below give surface area measured assuming the input image pixel resolution of n and input image size of 512 by 512\n",
    "    #other image sizes will make this calculation inaccurate\n",
    "    #change n in order to adjust pixel resolution within calculation\n",
    "    n = 0.25\n",
    "    #total lake surface area summed up from images interpreted by the model in ha\n",
    "    tlsa = summation(total_lake_sa)*n*n*0.0001\n",
    "    #total lake surface area based on masks prepared ahead of time in ha\n",
    "    omsa = summation(total_mask_sa)*n*n*0.0001\n",
    "    print(tlsa)\n",
    "    print(omsa)\n",
    "    #ratio of predicted to known surface area of the lake\n",
    "    print(tlsa/omsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebcf8d-db34-4c05-b280-da4c07f93e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lakes(data_path, model_pth, device, pixel_res):\n",
    "    model = UNet(in_channels=3, num_classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_pth, weights_only=True, map_location=torch.device(device)))\n",
    "    image_dataset = mldsjeziora_f(data_path)\n",
    "    lake_sa = []\n",
    "    for i in os.listdir(data_path):\n",
    "        images = []\n",
    "        pred_masks = []\n",
    "        total_lake_sa = 0\n",
    "        \n",
    "    for img in image_dataset:\n",
    "        img = img.float().to(device)\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        pred_mask = model(img)\n",
    "\n",
    "        img = img.squeeze(0).cpu().detach()\n",
    "        img = img.permute(1, 2, 0)\n",
    "        \n",
    "        pred_mask = pred_mask.squeeze(0).cpu().detach()\n",
    "        pred_mask = pred_mask.permute(1, 2, 0)\n",
    "        \n",
    "        pred_mask[pred_mask < 0.81]=0\n",
    "        pred_mask[pred_mask > 0.81]=1\n",
    "\n",
    "        images.append(img)\n",
    "        pred_masks.append(pred_mask)\n",
    "        pred_mask = pred_mask.numpy()\n",
    "        lake_sa = sum(pred_mask)\n",
    "        total_lake_sa = total_lake_sa + lake_sa\n",
    "        #total lake surface area summed up from images interpreted by the model in ha\n",
    "        tlsa = summation(total_lake_sa)*pixel_res*pixel_res*0.0001\n",
    "    var1 = tlsa.tolist()\n",
    "    var2 = tlsa[0]\n",
    "    \n",
    "    \n",
    "    images.extend(pred_masks)\n",
    "    fig = plt.figure()\n",
    "    for i in range(1, 2*len(image_dataset)+1):\n",
    "       fig.add_subplot(2, len(image_dataset), i)\n",
    "       plt.imshow(images[i-1], cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    return var2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee89702-e20c-4e38-b2dc-053964d14143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed27778-d05a-4fcf-bb12-a023fa34378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset class for final dataset without masks, used to move across the folder with images of lakes/lake, change paths in accordance with path example in comment within if statement below\n",
    "#in order to change file directories to move across, reccommended image resolution is 512 by 512 pixels for minimal loss of accuracy\n",
    "#all images will be resized to this resolution otherwise which may result in a varried amount of loss of accuracy\n",
    "class mldsjeziora_f(Dataset):\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        \n",
    "        #self.images = sorted([root_path+\"/test/image/\"+i for i in os.listdir(root_path+\"/test/image/\")])\n",
    "        self.images = sorted([root_path+i for i in os.listdir(root_path)])\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "\n",
    "        return self.transform(img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a38f2-2f67-4d55-89d3-0de499e26ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to create a roc curve and jaccard with test data prepared ahead of time, train mode 0\n",
    "def pred_show_image_grid_scikit(data_path, model_pth, device):\n",
    "    model = UNet(in_channels=3, num_classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_pth, weights_only=True, map_location=torch.device(device)))\n",
    "    image_dataset = mldsjeziora(data_path, test=True)\n",
    "    images = []\n",
    "    orig_masks = []\n",
    "    pred_masks = []\n",
    "    pred_mask_jacc = []\n",
    "    for img, orig_mask in image_dataset:\n",
    "        \n",
    "        img = img.float().to(device)\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        pred_mask = model(img)\n",
    "\n",
    "        img = img.squeeze(0).cpu().detach()\n",
    "        img = img.permute(1, 2, 0)\n",
    "        \n",
    "        pred_mask = pred_mask.squeeze(0).cpu().detach()\n",
    "        pred_mask = pred_mask.permute(1, 2, 0)\n",
    "        \n",
    "        orig_mask = orig_mask.cpu().detach()\n",
    "        orig_mask = orig_mask.permute(1, 2, 0)\n",
    "\n",
    "        images.append(img)\n",
    "        orig_masks.append(orig_mask)\n",
    "        pred_masks.append(pred_mask)\n",
    "\n",
    "    y_preds = np.array(pred_masks)\n",
    "    y_test = np.array(orig_masks)\n",
    "    \n",
    "    \n",
    "    fpr, tpr, roc_auc = roc_plot(y_test, y_preds)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver operating characteristic')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    images.extend(orig_masks)\n",
    "    images.extend(pred_masks)\n",
    "    fig = plt.figure()\n",
    "    for i in range(1, 3*len(image_dataset)+1):\n",
    "        fig.add_subplot(3, len(image_dataset), i)\n",
    "        plt.imshow(images[i-1], cmap=\"gray\")\n",
    "    plt.show()\n",
    "    jacccard_similarity(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f37381-450a-4cca-b279-02b49d32d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacccard_similarity(y, x):\n",
    "    x[x>=0.61] = 1\n",
    "    x[x<0.61] = 0\n",
    "    x = np.asarray(x, np.bool)\n",
    "    y = np.asarray(y, np.bool)\n",
    "    z = np.double(np.bitwise_and(x, y).sum()) / np.double(np.bitwise_or(x, y).sum())\n",
    "    print(\"jaccard = \")\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5eb25-6b26-4f14-a271-2dce9eaeccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_plot(ground_truth, pred):\n",
    "    ground_truth_labels = ground_truth.ravel()\n",
    "    ground_truth_labels = tf.cast(ground_truth_labels,tf.int8)\n",
    "    score_value = 1-pred.ravel()/255.0\n",
    "    #false positive rates, true positive rates\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( ground_truth_labels,score_value)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9c87c-8121-46bb-baa9-0eccd02ff6cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #environment variables changed to make sure tensorflow works correctly in case of no gpu device available\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "    os.environ['TF_ENABLE_ONEDNN_OPTS']='False'\n",
    "\n",
    "    #file paths\n",
    "    DATA_PATH = 'your_path_to_directories_containing_machine_learning_data'\n",
    "    DATA_PATH_PATCHES = \"your_path_to_directories_containing_patched_images_for_image_recognition_and_classification\"\n",
    "    MODEL_PATH = 'path_to_your_model'\n",
    "    MODEL_SAVE_PATH = 'path_where_your_model_will_be_saved'\n",
    "\n",
    "    #check cuda and set device to gpu mode if gpu available\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    #set train_mode to:\n",
    "    #0 to get roc curve and jaccard index  \n",
    "    #1 to resume machine learning process \n",
    "    #2 to calculate the dataset\n",
    "    train_mode = 2\n",
    "\n",
    "    #calculations below give surface area measured assuming the input image pixel resolution of n and input image size of 512 by 512\n",
    "    #other image sizes will make this calculation inaccurate\n",
    "    #change pixel_res[meters] in order to adjust pixel resolution within calculation\n",
    "    pixel_res = 0.25\n",
    "    \n",
    "    if train_mode == 0:\n",
    "        DATA_PATH = 'path_to_folder_with_test_data'\n",
    "        pred_show_image_grid_scikit(DATA_PATH, MODEL_PATH, device)\n",
    "    elif train_mode== 1:\n",
    "        LEARNING_RATE = 3e-4\n",
    "        BATCH_SIZE = 5\n",
    "        EPOCHS = 8\n",
    "        train_dataset = mldsjeziora(DATA_PATH)\n",
    "       \n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        train_dataset, val_dataset = random_split(train_dataset,[0.8, 0.2], generator=generator)\n",
    "    \n",
    "        train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True)\n",
    "        val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True)\n",
    "\n",
    "        model = UNet(in_channels=3, num_classes=1).to(device)\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, weights_only=True, map_location=torch.device(device)))\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "        for epoch in tqdm(range(EPOCHS)):\n",
    "            model.train()\n",
    "            train_running_loss = 0\n",
    "            for idx, img_mask in enumerate(tqdm(train_dataloader)):\n",
    "                img = img_mask[0].float().to(device)\n",
    "                mask = img_mask[1].float().to(device)\n",
    "\n",
    "                y_pred = model(img)\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                loss = criterion(y_pred, mask)\n",
    "                train_running_loss += loss.item()\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss = train_running_loss / (idx + 1)\n",
    "\n",
    "            model.eval()\n",
    "            val_running_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for idx, img_mask in enumerate(tqdm(val_dataloader)):\n",
    "                    img = img_mask[0].float().to(device)\n",
    "                    mask = img_mask[1].float().to(device)\n",
    "                \n",
    "                    y_pred = model(img)\n",
    "                    loss = criterion(y_pred, mask)\n",
    "\n",
    "                    val_running_loss += loss.item()\n",
    "    \n",
    "                val_loss = val_running_loss / (idx + 1)\n",
    "\n",
    "            print(\"-\"*30)\n",
    "            print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n",
    "            print(f\"Valid Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "            print(\"-\"*30)\n",
    "\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "    elif train_mode == 2:\n",
    "        limit = 0\n",
    "        lake_sa = []\n",
    "        lake_dir = []\n",
    "        for i in os.listdir(DATA_PATH_PATCHES):\n",
    "            directory = str(DATA_PATH_PATCHES + i)\n",
    "            lake_dir.append(directory)\n",
    "            print(directory)\n",
    "            var = calc_lakes((DATA_PATH_PATCHES + i + \"/\"), MODEL_PATH, device, pixel_res)\n",
    "            print(var)\n",
    "            lake_sa.append(var)\n",
    "            limit +=1\n",
    "            print(limit)\n",
    "        print(lake_sa)\n",
    "        print(lake_dir)\n",
    "        np.savetxt(\"specify_file_name_and_path_to_save_the_measured_surface_and_which_directory's_files_were_used_for_it\",[p for p in zip(lake_dir, lake_sa)] , delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10811dad-22db-4345-b8c5-9a1ad1b53ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
